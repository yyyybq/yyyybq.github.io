<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Baiqiao Yin</title>

    <meta name="author" content="Baiqiao Yin">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Baiqiao Yin„ÄåÂ∞πÊüè‰πî„Äç
                  </p>
                  <p>
                    Hi! I'm Baiqiao Yin. Most recently, I had the great fortune to work with <a href="https://limanling.github.io/">Manling Li</a> at Northwestern University, where we pushed the boundaries of spatial intelligence.
                     <!-- and collaborate with <a href="https://svl.stanford.edu/">Stanford SVL</a>. -->
                    Previously, I got my B.Eng. in Intelligent Science and Technology in SYSU, where I worked closely with <a href="https://www.cs.cmu.edu/~xiaodan1/">Xiaodan Liang</a>.<br>

                    Right now I'm spending my gap year at New York University as a research assistant, collaborating with <a href="https://yimingli-page.github.io/">Yiming Li</a> and <a href="https://www.sainingxie.com/">Saining Xie</a> on spatial intelligence.<br>

                    ‚≠êI am open for discussion/collaborations about spatial intelligence and looking for PhD opportunities (26 Fall). If you think there is anything interesting we can discuss, feel free to <a href="mailto:yinbaiqiao9@gmail.com">email</a> me!
                  </p>
                  <p style="text-align:center">
                    <a href="yinbaiqiao9@gmail.com">Email</a> /
                    <a href="https://scholar.google.com/citations?user=p8ZtdScAAAAJ&hl">Scholar</a> /
                    <a href="https://github.com/yyyybq/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%; width:37%; max-width:37%;">
                        <a href="images/baiqiao.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/baiqiao.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <h2>Internship</h2>
                  <ul style="list-style-type: none; padding-left: 0;">
                    <li>2024.07 - 2024.11, Shanghai AI Lab, Embodied AI Group. Mentor: <a href="https://sheldontsui.github.io/">Xudong Xu</a>. Topic: Indoor scene generation.</li>
                    <li>2023.05 - 2024.04, Peking  University(SZ), HRI Lab. Mentor: <a href="https://www.ece.pku.edu.cn/info/1046/2596.htm">Mengyuan Liu</a>. Topic: Human action recognition.</li>
                  </ul>
                </td>
              </tr>
              <tr>

                <td style="padding-top: 16px;">

                  <h2>üìùResearch</h2>

                  <p>

                  üí¨My research interests lie in spatial intelligence. Currently, my focus is on designing spatial intelligence agents with the following capabilities:

                  </p>

                  <ol>

                  <li>Spatial-Semantic Fusion</li>

                  <li>Spatial Mental Maniputation</li>

                  <li>Spatial Consistency Perception</li>

                  <li>Spatial Active Perception</li>

                  <li>Dynamic Spatial Understanding</li>

                  </ol>

                </td>

              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="bolt3d_stop()" onmouseover="bolt3d_start()" bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src='images/MindCube.png' width="160">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://mind-cube.github.io/">
                    <span class="papertitle">Spatial Mental Modeling from Limited Views</span>
                  </a>
                  <br>
                  <strong>Baiqiao Yin*</strong>,
                  <a href="https://szymanowiczs.github.io/">Qineng Wang*</a>,
                  <a href="https://williamzhangsjtu.github.io/">Pingyue Zhang</a>,
                  <a href="https://sterzhang.github.io/">Jianshu Zhang</a>,
                  <a href="https://jameskrw.github.io/">Kangrui Wang</a>,
                  <a href="https://zihanwang314.github.io/">Zihan Wang</a>,
                  <a href="https://jieyuz2.github.io/">Jieyu Zhang</a>,
                  <a href="https://keshik6.github.io/">Keshigeyan Chandrasegaran</a>,
                  <a href="https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/liu-han.html">Han Liu</a>,
                  <a href="https://ranjaykrishna.com/index.html">Ranjay Krishna</a>,
                  <a href="https://www.sainingxie.com/">Saining Xie</a>,
                  <a href="https://limanling.github.io/">Manling Li</a>,
                  <a href="https://jiajunwu.com/">Jiajun Wu</a>,
                  <a href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>
                  <br>
                  <em>(<font color="red"><em>spotlight</em></font>)ICCV Workshop on Structural Priors for Vision (SP4V)/ (<font color="red"><em>Oral</em></font>)ACMMM Workshop on Multimodal Foundation Models for Spatial Intelligence (MFMSI)</em>, 2025
                  <br>
                  <a href="https://mind-cube.github.io/">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2506.21458">arXiv</a>
                  <p></p>
                  <p>
                    Key Takeaway: Guiding VLMs to first generate cognitive maps, then reason upon them, is an effective approach to approximate spatial mental modeling with limited views.
                  </p>
                </td>
              </tr>


              <!-- <tr onmouseout="bolt3d_stop()" onmouseover="bolt3d_start()" bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src='images/MindCube.png' width="160">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://mind-cube.github.io/">
                    <span class="papertitle">Awesome Spatial Reasoning</span>
                  </a>
                  <em>arXiv</em>, 2025
                  <br>
                  <a href="https://mind-cube.github.io/">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2506.21458">arXiv</a>
                  <p></p>
                  <p>
                    Key Takeaway: Guiding VLMs to first generate cognitive maps, then reason upon them, is an effective approach to approximate spatial mental modeling with limited views.
                  </p>
                </td>
              </tr> -->
              <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src='images/skeleton2point.png' width="160">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://yyyybq.github.io/Skeleton2Point.github.io/">
                    <span class="papertitle">Skeleton2Point: Recognizing Skeleton-Based Actions as Point Clouds</span>
                  </a>
                  <br>
                  <strong>Baiqiao Yin</strong>,
                  Jiaying Lin</a>,
                  Jiajun Wen</a>,
                  Yue Li</a>,
                  <a href="https://github.com/liujf69">Jinfu Liu</a>,
                  Yanfei Wang</a>,
                  <a href="https://www.ece.pku.edu.cn/info/1046/2596.htm">Mengyuan Liu</a>
                  <br>
                  <em>IROS</em>, 2025(Oral) <font color=#FF8080></font>
                  <br>
                  <a href="https://yyyybq.github.io/Skeleton2Point.github.io/">project page</a>
                  /
                  <a href="https://github.com/yyyybq/Skeleton2Point/blob/main/Skeleton2Point.pdf">paper</a>
                  <p></p>
                  <p>
                    Regard skeleton joints as point cloud via incorporating the position information of skeletons into point cloud methods, demonstrating the validity of modeling position relationships with 3D coordinates.
                  </p>
                </td>
              </tr>

              <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src='images/Theater.png' width="160">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://howe140.github.io/theatergen.io/">
                    <span class="papertitle">TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation</span>
                  </a>
                  <br>
                  <a href="https://donahowe.github.io/"></a>Junhao Cheng</a>,
                  <strong>Baiqiao Yin</strong>,
                  Kaixin Cai</a>,
                  Minbin Huang</a>,
                  <a href="https://sysu-hcp.net/faculty/418.html">Hanhui Li</a>,
                  Yuxin He</a>,
                  Xi Lu</a>,
                  Yue Li</a>,
                  Yifei Li</a>,
                  Yiqiang Yan</a>,
                  <a href="https://www.cs.cmu.edu/~xiaodan1/">Xiaodan Liang</a>
                  <br>
                  <em>arxiv</em>, 2024 <font color=#FF8080></font>
                  <br>
                  <a href="https://howe140.github.io/theatergen.io/">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2404.18919">arxiv</a>
                  <p></p>
                  <p>
                    Theatergen can interact with users to consistently generate images over multiple Turns.
                  </p>
                </td>
              </tr>

              <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src='images/HDBN.png' width="160">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://github.com/liujf69/ICMEW2024-Track10">
                    <span class="papertitle">HDBN: A Novel Hybrid Dual-branch Network for Robust Skeleton-based Action Recognition</span>
                  </a>
                  <br>
                  <a href="https://github.com/liujf69">Jinfu Liu*</a>,
                  <strong>Baiqiao Yin*</strong>,
                  Jiaying Lin</a>,
                  Jiajun Wen</a>,
                  Yue Li</a>,
                  <a href="https://www.ece.pku.edu.cn/info/1046/2596.htm">Mengyuan Liu</a>
                  <br>
                  <em>ICME</em>, 2024 <font color=#FF8080></font>
                  <br>
                  <a href="https://github.com/liujf69/ICMEW2024-Track10">code</a>
                  /
                  <a href="https://ieeexplore.ieee.org/document/10645450">paper</a>
                  <p></p>
                  <p>
                    Benefits from the graph convolutional network's proficiency in handling graph-structured data and the powerful modeling capabilities of Transformers for global information.
                  </p>
                </td>
              </tr>


              <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <img src='images/LVLM_CL_intro_01.png' width="160">
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://openreview.net/forum?id=JIlIYIHMuv">
                    <span class="papertitle">LVLM-CL: Make Large Vision-Language Models Work Better Under Continual Learning Settings</span>
                  </a>
                  <br>
                  <strong>Baiqiao Yin</strong>,
                  <br>
                  <em>Tech Report</em> <font color=#FF8080></font>
                  <br>
                  <a href="https://openreview.net/forum?id=JIlIYIHMuv">paper</a>
                  <p></p>
                  <p>
                    Devise a task-specific continual learning setting especially for LVLMs by classifying the instruction tuning data for the second finetune process of LVLMs into several different tasks
                  </p>
                </td>
              </tr>


            </tbody>
          </table>
            </tbody>
          </table>

          <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <h2>üèÜHonors and Awards</h2>
                  <ul style="list-style-type: none; padding-left: 0;">
                    <li>2024.04: Champion of ICME Grand Challenge Multi-Modal Video Reasoning and Analyzing Competition.</li>
                    <li>2023.10: The Second Prize of Intelligent Robot Fighting and gaming competition.</li>
                    <li>2023.10: Academic Competition Scholarship.</li>
                    <li>2023.10: The Third Prize Scholarship.</li>
                    <li>2022.10: Academic Competition Scholarship.</li>
                    <li>2022.10: The Third Prize Scholarship.</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
</body>